<h1>Transformer Experiments</h1>

This repository contains two lightweight transformer-based pipelines, each built under limited computational resources. Both toy models demonstrate core methodologies—tokenisation, architecture setup, training, and inference—that can scale to high performance when provided with larger datasets and more compute.

For theory of Transformers and Large Language Models (LLMs), see our wiki. You can also visit the wiki directly at:

https://github.com/iffatAGheyas/transformer-experiments/wiki
For theory of Transformers and Large Language Models (LLMs), see our wiki.
![image](https://github.com/user-attachments/assets/de75a4c6-db19-4439-9035-7fc4a5ebf30b)
![image](https://github.com/user-attachments/assets/baa4ef66-161b-47b1-9c60-86a2e99594f5)
![image](https://github.com/user-attachments/assets/ad95e0fa-dc2c-4d89-bca0-fc277769f54c)

