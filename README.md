<h1>Transformer Experiments</h1>

This repository contains two lightweight transformer-based pipelines, each built under limited computational resources. Both toy models demonstrate core methodologies—tokenisation, architecture setup, training, and inference—that can scale to high performance when provided with larger datasets and more compute.

For theory of Transformers and Large Language Models (LLMs), see our wiki. You can also visit the wiki directly at:

https://github.com/iffatAGheyas/transformer-experiments/wiki

![image](https://github.com/user-attachments/assets/de75a4c6-db19-4439-9035-7fc4a5ebf30b)
![image](https://github.com/user-attachments/assets/acd56431-9b36-4064-9416-1d5a97e7a9ed)
Note: This model trains on ~400 examples across 50 classes for only 5 epochs (CPU-only), resulting in limited accuracy. With more data, longer training, and GPU acceleration, the same attention-based approach can achieve strong performance on audio classification.

---:** This model trains on ~400 examples across 50 classes for only 5 epochs (CPU-only), resulting in limited accuracy. With more data, longer training, and GPU acceleration, the same attention-based approach can achieve strong performance on audio classification.

![image](https://github.com/user-attachments/assets/ad95e0fa-dc2c-4d89-bca0-fc277769f54c)

