{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c588556e-488c-4ebe-8e8f-a56614e17d2e",
   "metadata": {},
   "source": [
    "# Audio Transformer Pipeline\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow for audio classification using a “TinyAST” (Audio Spectrogram Transformer) on the ESC-50 dataset. The code is organised into the following sections:\n",
    "\n",
    "1. **Setup & Dependencies**  \n",
    "   - Install necessary Python packages.  \n",
    "   - Define global paths and parameters.  \n",
    "\n",
    "2. **Dataset Download**  \n",
    "   - Clone the ESC-50 GitHub repository into `data/raw/ESC-50-master/` (if not already present).  \n",
    "   - Brief overview of ESC-50 and its 50 environmental sound classes.\n",
    "\n",
    "3. **Preprocessing**  \n",
    "   - Load ESC-50 metadata and raw `.wav` files.  \n",
    "   - Compute 64-bin Mel spectrograms (16 kHz, 1024-point FFT, 512 hop).  \n",
    "   - Convert to dB scale, normalise per clip, and save as `.npy` in `data/processed/`.  \n",
    "   - Generate a `metadata.csv` linking each `.npy` to its class label.\n",
    "\n",
    "4. **Model Training & Hold-out Evaluation**  \n",
    "   - Read processed metadata and merge with original labels.  \n",
    "   - Reserve 10 random clips as a hold-out test set.  \n",
    "   - Define a custom Dataset that crops/pads spectrograms to 1×64×64.  \n",
    "   - Build and train a lightweight Vision Transformer (`TinyAST`) for **5 epochs**.  \n",
    "   - Evaluate on the 10 hold-out samples and print actual vs predicted labels.  \n",
    "   - **Note:** Because we’re training on only ~400 examples (50 classes) and just 5 epochs due to CPU constraints, overall accuracy will be low. With more training data, longer epochs and GPU acceleration, this approach scales to much higher performance.\n",
    "\n",
    "5. **Batch Inference (Optional)**  \n",
    "   - Load the trained model weights.  \n",
    "   - Run inference on any WAV files in `data/test/` and visualise or print predictions.\n",
    "\n",
    "Use this notebook as a template for experimenting with attention-based audio models—swap in your own datasets, adjust the model configuration or training schedule, and leverage more compute to improve accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4595e0-73bc-4a3b-83b2-6eaacfdd2285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "%pip install librosa soundfile pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b76c51-d37f-41da-9f31-f4056605d5b2",
   "metadata": {},
   "source": [
    "## Download and Prepare the ESC-50 Dataset\n",
    "\n",
    "This cell ensures that the ESC-50 environmental sound dataset is available under `data/raw/ESC-50-master`. If the folder doesn’t already exist, it will:\n",
    "\n",
    "1. Create the `data/raw` directory (if needed).  \n",
    "2. Clone the official ESC-50 repository from GitHub into `data/raw/ESC-50-master`.\n",
    "\n",
    "### About ESC-50\n",
    "\n",
    "ESC-50 is a benchmark dataset for environmental sound classification. It contains 2,000 five-second audio clips evenly distributed across 50 semantic classes (e.g. dog bark, rain, siren, clock tick). Each clip is annotated with a category label, making it ideal for prototyping and evaluating audio classification pipelines. You can find the original repository and detailed documentation here: https://github.com/karolpiczak/ESC-50.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d41482c3-679c-4533-b4f7-f44c3098aaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ESC-50 into data\\raw\\ESC-50-master …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'data\\raw\\ESC-50-master'...\n",
      "Updating files:  10% (205/2011)\n",
      "Updating files:  11% (222/2011)\n",
      "Updating files:  12% (242/2011)\n",
      "Updating files:  13% (262/2011)\n",
      "Updating files:  14% (282/2011)\n",
      "Updating files:  15% (302/2011)\n",
      "Updating files:  16% (322/2011)\n",
      "Updating files:  17% (342/2011)\n",
      "Updating files:  18% (362/2011)\n",
      "Updating files:  19% (383/2011)\n",
      "Updating files:  20% (403/2011)\n",
      "Updating files:  20% (409/2011)\n",
      "Updating files:  21% (423/2011)\n",
      "Updating files:  22% (443/2011)\n",
      "Updating files:  23% (463/2011)\n",
      "Updating files:  24% (483/2011)\n",
      "Updating files:  25% (503/2011)\n",
      "Updating files:  26% (523/2011)\n",
      "Updating files:  27% (543/2011)\n",
      "Updating files:  28% (564/2011)\n",
      "Updating files:  29% (584/2011)\n",
      "Updating files:  30% (604/2011)\n",
      "Updating files:  30% (611/2011)\n",
      "Updating files:  31% (624/2011)\n",
      "Updating files:  32% (644/2011)\n",
      "Updating files:  33% (664/2011)\n",
      "Updating files:  34% (684/2011)\n",
      "Updating files:  35% (704/2011)\n",
      "Updating files:  36% (724/2011)\n",
      "Updating files:  37% (745/2011)\n",
      "Updating files:  38% (765/2011)\n",
      "Updating files:  39% (785/2011)\n",
      "Updating files:  40% (805/2011)\n",
      "Updating files:  40% (808/2011)\n",
      "Updating files:  41% (825/2011)\n",
      "Updating files:  42% (845/2011)\n",
      "Updating files:  43% (865/2011)\n",
      "Updating files:  44% (885/2011)\n",
      "Updating files:  45% (905/2011)\n",
      "Updating files:  46% (926/2011)\n",
      "Updating files:  47% (946/2011)\n",
      "Updating files:  48% (966/2011)\n",
      "Updating files:  49% (986/2011)\n",
      "Updating files:  50% (1006/2011)\n",
      "Updating files:  50% (1017/2011)\n",
      "Updating files:  51% (1026/2011)\n",
      "Updating files:  52% (1046/2011)\n",
      "Updating files:  53% (1066/2011)\n",
      "Updating files:  54% (1086/2011)\n",
      "Updating files:  55% (1107/2011)\n",
      "Updating files:  56% (1127/2011)\n",
      "Updating files:  57% (1147/2011)\n",
      "Updating files:  58% (1167/2011)\n",
      "Updating files:  59% (1187/2011)\n",
      "Updating files:  60% (1207/2011)\n",
      "Updating files:  60% (1222/2011)\n",
      "Updating files:  61% (1227/2011)\n",
      "Updating files:  62% (1247/2011)\n",
      "Updating files:  63% (1267/2011)\n",
      "Updating files:  64% (1288/2011)\n",
      "Updating files:  65% (1308/2011)\n",
      "Updating files:  66% (1328/2011)\n",
      "Updating files:  67% (1348/2011)\n",
      "Updating files:  68% (1368/2011)\n",
      "Updating files:  69% (1388/2011)\n",
      "Updating files:  70% (1408/2011)\n",
      "Updating files:  70% (1420/2011)\n",
      "Updating files:  71% (1428/2011)\n",
      "Updating files:  72% (1448/2011)\n",
      "Updating files:  73% (1469/2011)\n",
      "Updating files:  74% (1489/2011)\n",
      "Updating files:  75% (1509/2011)\n",
      "Updating files:  76% (1529/2011)\n",
      "Updating files:  77% (1549/2011)\n",
      "Updating files:  78% (1569/2011)\n",
      "Updating files:  79% (1589/2011)\n",
      "Updating files:  80% (1609/2011)\n",
      "Updating files:  80% (1623/2011)\n",
      "Updating files:  81% (1629/2011)\n",
      "Updating files:  82% (1650/2011)\n",
      "Updating files:  83% (1670/2011)\n",
      "Updating files:  84% (1690/2011)\n",
      "Updating files:  85% (1710/2011)\n",
      "Updating files:  86% (1730/2011)\n",
      "Updating files:  87% (1750/2011)\n",
      "Updating files:  88% (1770/2011)\n",
      "Updating files:  89% (1790/2011)\n",
      "Updating files:  90% (1810/2011)\n",
      "Updating files:  90% (1822/2011)\n",
      "Updating files:  91% (1831/2011)\n",
      "Updating files:  92% (1851/2011)\n",
      "Updating files:  93% (1871/2011)\n",
      "Updating files:  94% (1891/2011)\n",
      "Updating files:  95% (1911/2011)\n",
      "Updating files:  96% (1931/2011)\n",
      "Updating files:  97% (1951/2011)\n",
      "Updating files:  98% (1971/2011)\n",
      "Updating files:  99% (1991/2011)\n",
      "Updating files: 100% (2011/2011)\n",
      "Updating files: 100% (2011/2011), done.\n"
     ]
    }
   ],
   "source": [
    "#1. Clone ESC-50 into data/raw\n",
    "import os\n",
    "\n",
    "# Where we expect the raw ESC-50 repo to live:\n",
    "RAW_ESC50 = os.path.join(\"data\", \"raw\", \"ESC-50-master\")\n",
    "\n",
    "if not os.path.isdir(RAW_ESC50):\n",
    "    print(f\"Downloading ESC-50 into {RAW_ESC50} …\")\n",
    "    os.makedirs(os.path.dirname(RAW_ESC50), exist_ok=True)\n",
    "    # Clone the GitHub repo into that location\n",
    "    !git clone https://github.com/karolpiczak/ESC-50.git \"{RAW_ESC50}\"\n",
    "else:\n",
    "    print(\"ESC-50 already present at\", RAW_ESC50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99560a2a-d019-47e7-808c-6b91be161a94",
   "metadata": {},
   "source": [
    "## Preprocessing ESC-50 into Normalised Mel-Spectrograms\n",
    "\n",
    "This cell performs the end-to-end conversion of the raw ESC-50 audio clips into training-ready NumPy arrays (*.npy*) and generates a corresponding metadata CSV. In particular, it:\n",
    "\n",
    "1. **Sets up project paths** without relying on the current working directory, ensuring reproducibility across environments.\n",
    "2. **Verifies** that the raw ESC-50 audio files and their metadata CSV are present in `data/raw/ESC-50-master/`.\n",
    "3. **Loads** the ESC-50 metadata table, which contains filenames, class indexes, and class names.\n",
    "4. **Defines preprocessing parameters**:\n",
    "\n",
    "   * Sampling rate: 16 kHz\n",
    "   * Mel filterbanks: 64 bins\n",
    "   * FFT window: 1024 samples\n",
    "   * Hop length: 512 samples\n",
    "5. **Iterates** through each clip and:\n",
    "\n",
    "   * Loads and resamples the waveform to 16 kHz.\n",
    "   * Computes a 64-bin mel-spectrogram.\n",
    "   * Converts the power spectrogram to decibel (dB) scale.\n",
    "   * Normalises each clip to zero mean and unit variance.\n",
    "   * Saves the result as a 2D NumPy array in `data/processed/`.\n",
    "6. **Accumulates** a new metadata DataFrame mapping each `.npy` file to its numeric label.\n",
    "7. **Writes** this processed metadata to `data/processed/metadata.csv` for seamless integration with downstream training and evaluation steps.\n",
    "\n",
    "After running this cell, all of the raw audio is represented as uniform, normalised spectrogram arrays, and you have a single CSV that links each processed file to its class label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "357b4368-31bf-4a3f-b4aa-6afd6bdecf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 mel-spectrograms to 'C:\\Users\\IAGhe\\OneDrive\\Documents\\Learning\\portfolio\\audio_spectogram_transformer\\data\\processed'\n"
     ]
    }
   ],
   "source": [
    "# Full Preprocessing Pipeline with Path Check\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Workaround for numpy 1.24+ removing np.complex\n",
    "np.complex = complex\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# 1. Define project folder (no chdir, so it won’t fail in this environment)\n",
    "BASE = r\"C:\\Users\\IAGhe\\OneDrive\\Documents\\Learning\\portfolio\\audio_spectogram_transformer\"\n",
    "\n",
    "# 2. Define raw & processed data paths\n",
    "RAW_DIR  = os.path.join(BASE, \"data\", \"raw\", \"ESC-50-master\", \"audio\")\n",
    "META_CSV = os.path.join(BASE, \"data\", \"raw\", \"ESC-50-master\", \"meta\",  \"esc50.csv\")\n",
    "PROC_DIR = os.path.join(BASE, \"data\", \"processed\")\n",
    "os.makedirs(PROC_DIR, exist_ok=True)\n",
    "\n",
    "# 3. Check that raw data is present\n",
    "if not os.path.isdir(RAW_DIR) or not os.path.isfile(META_CSV):\n",
    "    print(\"ERROR: ESC-50 data not found.\")\n",
    "    print(\"Please ensure the folder structure is:\")\n",
    "    print(\"  audio_spectogram_transformer/\")\n",
    "    print(\"    data/raw/ESC-50-master/audio/*.wav\")\n",
    "    print(\"    data/raw/ESC-50-master/meta/esc50.csv\")\n",
    "else:\n",
    "    # 4. Load ESC-50 metadata\n",
    "    meta = pd.read_csv(META_CSV)\n",
    "\n",
    "    # 5. Preprocessing parameters\n",
    "    TARGET_SR = 16_000\n",
    "    N_MELS    = 64\n",
    "    N_FFT     = 1024\n",
    "    HOP_LEN   = 512\n",
    "\n",
    "    # 6. Process each file\n",
    "    processed_rows = []\n",
    "    for _, row in meta.iterrows():\n",
    "        fname   = row[\"filename\"]\n",
    "        label   = int(row[\"target\"])\n",
    "        wavpath = os.path.join(RAW_DIR, fname)\n",
    "\n",
    "        # a) Load & resample\n",
    "        y, sr = librosa.load(wavpath, sr=TARGET_SR, mono=True)\n",
    "\n",
    "        # b) Compute mel-spectrogram\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=y,\n",
    "            sr=TARGET_SR,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LEN,\n",
    "            n_mels=N_MELS\n",
    "        )\n",
    "\n",
    "        # c) Convert to log scale (dB)\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "        # d) Normalise per-clip\n",
    "        mel_db_norm = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n",
    "\n",
    "        # e) Save as .npy\n",
    "        out_name = os.path.splitext(fname)[0] + \".npy\"\n",
    "        out_path = os.path.join(PROC_DIR, out_name)\n",
    "        np.save(out_path, mel_db_norm.astype(np.float32))\n",
    "\n",
    "        processed_rows.append({\"npy_file\": out_name, \"label\": label})\n",
    "\n",
    "    # 7. Save processed metadata\n",
    "    proc_meta = pd.DataFrame(processed_rows)\n",
    "    proc_meta.to_csv(os.path.join(PROC_DIR, \"metadata.csv\"), index=False)\n",
    "\n",
    "    print(f\"Saved {len(proc_meta)} mel-spectrograms to '{PROC_DIR}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c5d03-9efb-4b6d-983a-ef49b826847c",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Pipeline\n",
    "\n",
    "This cell implements an end-to-end workflow for training and evaluating our `TinyAST` audio transformer on the preprocessed ESC-50 mel-spectrograms, with a 10-sample hold-out test:\n",
    "\n",
    "1. **Paths & Hyperparameters**: Sets project directories and key settings (e.g. number of mel bins, fixed time-frames, batch size, learning rate, epochs, and test set size).\n",
    "2. **Metadata Loading & Merge**: Reads the processed spectrogram metadata and original ESC-50 labels, then combines them to associate each `.npy` file with its human-readable category.\n",
    "3. **Hold-out Split**: Randomly selects 10 examples as an unseen test set (seeded for reproducibility) and uses the rest for training.\n",
    "4. **Fixed-Sized Dataset**: Defines a `FixedSpecDataset` that crops or zero-pads each spectrogram to a fixed shape of `1×64×64` so it matches the transformer’s expected input size.\n",
    "5. **Model Definition**: Instantiates a `TinyAST` model (a lightweight Vision Transformer adapted for single-channel, 64×64 inputs) using the Hugging Face `ViTModel` backbone.\n",
    "6. **Training Loop**: Trains the model for the specified number of epochs using cross-entropy loss and the Adam optimizer, printing loss per epoch.\n",
    "7. **Evaluation on Hold-out Set**: Runs inference on the 10 held-out clips and prints each file’s actual category alongside the model’s predicted category—no visualisation, just a clear text report.\n",
    "\n",
    "This pipeline demonstrates the full machine learning workflow from data loading, fixed-size preprocessing, model training, and hold-out evaluation in a single Jupyter cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86420e26-7740-48a6-96b5-761b01755317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 — Loss: 3.8314\n",
      "Epoch 2/5 — Loss: 3.5725\n",
      "Epoch 3/5 — Loss: 3.4761\n",
      "Epoch 4/5 — Loss: 3.3440\n",
      "Epoch 5/5 — Loss: 3.2762\n",
      "\n",
      "Hold-out Test Set Results:\n",
      "4-161579-A-40.wav     actual = helicopter       predicted = clock_alarm\n",
      "1-47714-A-16.wav      actual = wind             predicted = helicopter\n",
      "1-17092-A-27.wav      actual = brushing_teeth   predicted = crying_baby\n",
      "4-201300-A-31.wav     actual = mouse_click      predicted = can_opening\n",
      "2-139748-B-15.wav     actual = water_drops      predicted = fireworks\n",
      "2-118625-A-30.wav     actual = door_wood_knock  predicted = door_wood_knock\n",
      "2-109231-B-9.wav      actual = crow             predicted = pouring_water\n",
      "1-57163-A-38.wav      actual = clock_tick       predicted = clock_alarm\n",
      "4-198360-A-49.wav     actual = hand_saw         predicted = toilet_flush\n",
      "1-43807-A-47.wav      actual = airplane         predicted = train\n"
     ]
    }
   ],
   "source": [
    "# %% Full Training + 10-Sample Test Pipeline (Fixed 64×64 Inputs)\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTConfig, ViTModel\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "# 1) Paths & hyperparameters\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "BASE         = r\"C:\\Users\\IAGhe\\OneDrive\\Documents\\Learning\\portfolio\\audio_spectogram_transformer\"\n",
    "PROC_DIR     = os.path.join(BASE, \"data\", \"processed\")\n",
    "RAW_META_CSV = os.path.join(BASE, \"data\", \"raw\", \"ESC-50-master\", \"meta\", \"esc50.csv\")\n",
    "\n",
    "N_MELS      = 64\n",
    "FIXED_T     = 64\n",
    "BATCH_SIZE  = 8\n",
    "EPOCHS      = 5\n",
    "LR          = 1e-3\n",
    "TEST_SIZE   = 10\n",
    "SEED        = 42\n",
    "device      = torch.device(\"cpu\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "# 2) Load processed & raw metadata; merge for categories\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "proc_meta = pd.read_csv(os.path.join(PROC_DIR, \"metadata.csv\"))\n",
    "proc_meta[\"filename\"] = proc_meta[\"npy_file\"].str.replace(\".npy\", \".wav\")\n",
    "raw_meta  = pd.read_csv(RAW_META_CSV)\n",
    "meta = proc_meta.merge(raw_meta[[\"filename\",\"category\",\"target\"]], on=\"filename\")\n",
    "\n",
    "# Build a lookup for predictions\n",
    "label_map = dict(zip(raw_meta[\"target\"], raw_meta[\"category\"]))\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "# 3) Hold-out split\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "random.seed(SEED)\n",
    "test_idxs     = random.sample(list(meta.index), TEST_SIZE)\n",
    "test_meta     = meta.loc[test_idxs].reset_index(drop=True)\n",
    "trainval_meta = meta.drop(test_idxs).reset_index(drop=True)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "# 4) Dataset that crops/pads to [1×N_MELS×FIXED_T]\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "class FixedSpecDataset(Dataset):\n",
    "    def __init__(self, df, npy_dir, fixed_t):\n",
    "        self.df      = df\n",
    "        self.npy_dir = npy_dir\n",
    "        self.fixed_t = fixed_t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row  = self.df.iloc[i]\n",
    "        spec = np.load(os.path.join(self.npy_dir, row.npy_file))  # (n_mels, T)\n",
    "        spec = spec[np.newaxis, :, :]                            # (1, n_mels, T)\n",
    "        _, m, t = spec.shape\n",
    "\n",
    "        if t >= self.fixed_t:\n",
    "            spec = spec[:, :, :self.fixed_t]\n",
    "        else:\n",
    "            pad = np.zeros((1, m, self.fixed_t - t), dtype=spec.dtype)\n",
    "            spec = np.concatenate([spec, pad], axis=2)\n",
    "\n",
    "        return torch.from_numpy(spec).float(), torch.tensor(row.target, dtype=torch.long)\n",
    "\n",
    "train_ds = FixedSpecDataset(trainval_meta, PROC_DIR, FIXED_T)\n",
    "test_ds  = FixedSpecDataset(test_meta,     PROC_DIR, FIXED_T)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=1,         shuffle=False)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "# 5) Define TinyAST model\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "cfg = ViTConfig(\n",
    "    image_size=N_MELS,\n",
    "    patch_size=16,\n",
    "    num_channels=1,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=256,\n",
    "    num_labels=len(raw_meta[\"target\"].unique())\n",
    ")\n",
    "\n",
    "class TinyAST(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.vit      = ViTModel(config)\n",
    "        self.cls_head = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.vit(pixel_values=x)  # expects [B,1,64,64]\n",
    "        return self.cls_head(out.pooler_output)\n",
    "\n",
    "model = TinyAST(cfg).to(device)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "# 6) Training\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "optim   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optim.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss   = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} — Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "# 7) Evaluate on hold-out: print actual vs predicted\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "print(\"\\nHold-out Test Set Results:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (spec, actual_lbl) in enumerate(test_loader):\n",
    "        spec       = spec.to(device)\n",
    "        pred_lbl   = model(spec).argmax(dim=-1).item()\n",
    "        fname      = test_meta.loc[i, \"filename\"]\n",
    "        actual_cat = test_meta.loc[i, \"category\"]\n",
    "        pred_cat   = label_map[pred_lbl]\n",
    "        print(f\"{fname:20s}  actual = {actual_cat:<15s}  predicted = {pred_cat}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c24ba5-be28-4185-a85e-20ae50f0484c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
