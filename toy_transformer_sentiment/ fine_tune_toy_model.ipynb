{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65484ed9-7841-450b-b5f5-d0aaec8796c5",
   "metadata": {},
   "source": [
    "# Toy Sentiment Classification with DistilBERT\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "1. **Pull the SST-2 dataset** from the GLUE benchmark:\n",
    "   - **Train split:** 67 349 examples  \n",
    "   - **Validation split:** 872 examples  \n",
    "   - **Test split:** 1 821 examples  \n",
    "\n",
    "2. **Subsample** due to limited compute:\n",
    "   - **Training/Validation:** 40 examples (20 positive, 20 negative) drawn from the original train split, then split 80/20 into 32 training and 8 validation examples.  \n",
    "   - **Held-out Test:** 200 examples (100 positive, 100 negative) also drawn from the train split for final evaluation.\n",
    "\n",
    "3. **Fine-tune a DistilBERT classifier** (`distilbert-base-uncased`) on our tiny dataset:\n",
    "   - Tokenise sentences to a maximum length of 64 tokens.  \n",
    "   - Add a small MLP head (768 → GELU → dropout → 2 logits) on top of the 6-layer, 768-hidden‐size DistilBERT backbone.  \n",
    "   - Train for 5 epochs with a batch size of 4, learning rate 2e-5, on CPU only.\n",
    "\n",
    "4. **Save the resulting model** and tokenizer to  distilbert_finetuned/. (this directory contains `pytorch_model.bin`, `config.json`, `vocab.txt`, etc.)\n",
    "\n",
    "5. **Evaluate** on both the 8-example validation set (during training) and the 200-example held-out test set.\n",
    "\n",
    "> **Note**: This is a **toy example** to illustrate the full pipeline. In a production scenario you would fine-tune on the full SST-2 train split (≈67 k examples), use more epochs, a larger batch size or GPU acceleration, and tune hyperparameters to maximize real-world performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c290301-e0a6-4273-9ce0-840b3be88474",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers datasets torch scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5965ce39-b3e1-4082-b3da-d2da9a38dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30b65279-20c0-4018-bc3e-c04d75063d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "373c8208-a40e-44aa-a2d8-2baac592124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d02180cf-804a-4e73-a3cc-86d2f3b542ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002792f6-ce09-4db2-a0b8-ea65b5368d0e",
   "metadata": {},
   "source": [
    "## Load and Inspect the SST-2 Sentiment Dataset\n",
    "\n",
    "This cell performs the following steps in preparation for model fine-tuning:\n",
    "\n",
    "1. **Import the dataset loader**  \n",
    "   Brings in the `load_dataset` function from Hugging Face’s Datasets library.\n",
    "\n",
    "2. **Download the SST-2 data**  \n",
    "   Fetches the Stanford Sentiment Treebank (SST-2) from the GLUE benchmark, including `train`, `validation`, and `test` splits.\n",
    "\n",
    "3. **Show available splits**  \n",
    "   Prints out a summary of each split (its name and number of examples) so you can verify that the data was loaded correctly.\n",
    "\n",
    "4. **Select the training split**  \n",
    "   Extracts the `train` portion of the dataset (approximately 67 000 sentences) for use in fine-tuning.\n",
    "\n",
    "5. **Display dataset details**  \n",
    "   - Prints the total number of training examples.  \n",
    "   - Shows the very first example (a dictionary containing a sentence and its sentiment label) so you can confirm the data format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ded0e8-d5e2-4910-a1b4-2a38d50d0ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n",
      "Number of training examples: 67349\n",
      "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\n"
     ]
    }
   ],
   "source": [
    "#pull in SST-2 from the GLUE benchmark\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Download the SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# 2. Inspect the splits\n",
    "print(dataset)\n",
    "\n",
    "# 3. Grab the training split\n",
    "train_ds = dataset[\"train\"]\n",
    "print(f\"Number of training examples: {len(train_ds)}\")\n",
    "print(train_ds[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dabe3e7-28dd-4fff-945a-6e53e85d4b36",
   "metadata": {},
   "source": [
    "## Create and Save Toy Training and Test Sets\n",
    "\n",
    "This cell builds small, balanced subsets from the full SST-2 training data and writes them to Excel files:\n",
    "\n",
    "1. **Import necessary libraries**  \n",
    "   - `random` for reproducible sampling  \n",
    "   - Hugging Face’s `load_dataset` to retrieve SST-2  \n",
    "   - `pandas` to manipulate and save the data  \n",
    "\n",
    "2. **Load the complete SST-2 training split**  \n",
    "   Retrieves all ~67 000 examples from the GLUE SST-2 “train” split.\n",
    "\n",
    "3. **Gather positive and negative indices**  \n",
    "   Iterates through the dataset to collect two lists of example indices:  \n",
    "   - `pos_idxs` for positive sentences (`label == 1`)  \n",
    "   - `neg_idxs` for negative sentences (`label == 0`)  \n",
    "\n",
    "4. **Sample a small training set**  \n",
    "   - Seeds the random number generator for reproducibility.  \n",
    "   - Randomly selects 20 positive and 20 negative indices.  \n",
    "   - Combines them into `train_idxs`.\n",
    "\n",
    "5. **Sample a held-out test set**  \n",
    "   - Removes the training indices from the original pools.  \n",
    "   - Randomly picks 100 new positive and 100 new negative indices.  \n",
    "   - Combines them into `test_idxs`.\n",
    "\n",
    "6. **Extract the subsets**  \n",
    "   Uses `dataset.select()` to pull out only the sampled examples for both the training and test sets.\n",
    "\n",
    "7. **Convert to pandas DataFrames and shuffle**  \n",
    "   - Builds a DataFrame with columns `sentence` and `label` for each split.  \n",
    "   - Shuffles the rows and resets the index for randomness.\n",
    "\n",
    "8. **Save to Excel files**  \n",
    "   - Writes the 40-example training set to **`training.xlsx`**  \n",
    "   - Writes the 200-example test set to **`test.xlsx`**  \n",
    "\n",
    "9. **Confirm saving**  \n",
    "   Prints the number of rows saved in each file to verify that the correct number of examples were written.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad61f13-e6be-4f5b-8c07-918866e4d2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " • training.xlsx (40 rows)\n",
      " • test.xlsx (200 rows)\n"
     ]
    }
   ],
   "source": [
    "# training and test datasets\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Load the full SST-2 train split\n",
    "dataset = load_dataset(\"glue\", \"sst2\")[\"train\"]\n",
    "\n",
    "# 2) Build index lists\n",
    "pos_idxs = [i for i, ex in enumerate(dataset) if ex[\"label\"] == 1]\n",
    "neg_idxs = [i for i, ex in enumerate(dataset) if ex[\"label\"] == 0]\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# 3) Sample for TRAIN: 20 pos, 20 neg\n",
    "train_pos = random.sample(pos_idxs, 20)\n",
    "train_neg = random.sample(neg_idxs, 20)\n",
    "train_idxs = train_pos + train_neg\n",
    "\n",
    "# 4) Remove those from pools, then sample for TEST: 100 pos, 100 neg\n",
    "remaining_pos = [i for i in pos_idxs if i not in train_pos]\n",
    "remaining_neg = [i for i in neg_idxs if i not in train_neg]\n",
    "\n",
    "test_pos = random.sample(remaining_pos, 100)\n",
    "test_neg = random.sample(remaining_neg, 100)\n",
    "test_idxs = test_pos + test_neg\n",
    "\n",
    "# 5) Select subsets\n",
    "train_ds = dataset.select(train_idxs)\n",
    "test_ds  = dataset.select(test_idxs)\n",
    "\n",
    "# 6) Convert to DataFrame\n",
    "train_df = pd.DataFrame({\n",
    "    \"sentence\": train_ds[\"sentence\"],\n",
    "    \"label\":    train_ds[\"label\"]\n",
    "}).sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"sentence\": test_ds[\"sentence\"],\n",
    "    \"label\":    test_ds[\"label\"]\n",
    "}).sample(frac=1, random_state=42).reset_index(drop=True)   # shuffle\n",
    "\n",
    "# 7) Save to Excel\n",
    "train_df.to_excel(\"training.xlsx\", index=False)\n",
    "test_df.to_excel(\"test.xlsx\",      index=False)\n",
    "\n",
    "print(f\"Saved:\\n • training.xlsx ({len(train_df)} rows)\\n • test.xlsx ({len(test_df)} rows)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93402344-1f0b-46eb-b46e-fc5747aa2e37",
   "metadata": {},
   "source": [
    "## Load and Split the 40-Example Training Set into Train/Validation\n",
    "\n",
    "This cell prepares our small toy dataset for fine-tuning by:\n",
    "\n",
    "1. **Importing libraries**  \n",
    "   - `pandas` for reading Excel files  \n",
    "   - Hugging Face’s `Dataset` class for easy downstream processing  \n",
    "\n",
    "2. **Loading the 40-example CSV**  \n",
    "   - Reads **`training.xlsx`** into a pandas DataFrame (`train_df`) containing exactly 40 rows.\n",
    "\n",
    "3. **Converting to a Hugging Face Dataset**  \n",
    "   - Wraps the DataFrame with `Dataset.from_pandas()` to get `ds40`, enabling `.map()`, `.train_test_split()`, and other Dataset operations.\n",
    "\n",
    "4. **Cleaning up index columns**  \n",
    "   - Removes any auto-generated pandas index columns (names starting with `__`) so only our original `sentence` and `label` fields remain.\n",
    "\n",
    "5. **Splitting into train and validation**  \n",
    "   - Uses an 80/20 split (`test_size=0.2`) to create:  \n",
    "     - **`train_ds`** with 32 examples for fine-tuning  \n",
    "     - **`val_ds`** with 8 examples for in-training validation  \n",
    "   - A fixed seed guarantees reproducible splits.\n",
    "\n",
    "6. **Confirmation printout**  \n",
    "   - Prints the sizes of each split to ensure we have the expected 32 training and 8 validation examples.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e005a2d-31ad-479f-b1fd-ef52453a6e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 examples, validate on 8 examples\n"
     ]
    }
   ],
   "source": [
    "#1. Load only the 40-example training file and split into training/validation\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the 40-example training set\n",
    "train_df = pd.read_excel(\"training.xlsx\")\n",
    "ds40     = Dataset.from_pandas(train_df)\n",
    "\n",
    "# Remove any pandas index column\n",
    "ds40 = ds40.remove_columns([c for c in ds40.column_names if c.startswith(\"__\")])\n",
    "\n",
    "# Split: 80% train (32 examples), 20% validation (8 examples)\n",
    "split = ds40.train_test_split(test_size=0.2, seed=42)\n",
    "train_ds = split[\"train\"]\n",
    "val_ds   = split[\"test\"]\n",
    "\n",
    "print(f\"Train on {len(train_ds)} examples, validate on {len(val_ds)} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036f6e1-5f47-45bd-8553-5737e91075b5",
   "metadata": {},
   "source": [
    "## Load the Held-Out Test Set for Final Evaluation\n",
    "\n",
    "This cell performs the following steps to prepare the test data for a one-time evaluation after training:\n",
    "\n",
    "1. **Read the 200-example test file**  \n",
    "   Loads **`test.xlsx`** into a pandas DataFrame (`test_df`) containing 200 sentences and their labels.\n",
    "\n",
    "2. **Convert to a Hugging Face Dataset**  \n",
    "   Wraps the DataFrame with `Dataset.from_pandas()` to create `test_ds`, allowing it to be processed like our other datasets.\n",
    "\n",
    "3. **Clean up any stray index columns**  \n",
    "   Removes any auto-generated pandas index columns (names starting with `__`) so that only the original `sentence` and `label` fields remain.\n",
    "\n",
    "4. **Confirm dataset size**  \n",
    "   Prints out the total number of examples in the held-out test set (should be 200) to verify that it loaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ea0eff-37ac-4101-851b-eef5685773e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Held-out test set size: 200\n"
     ]
    }
   ],
   "source": [
    "#2. Load the 200-example test set for final evaluation only (later)\n",
    "test_df = pd.read_excel(\"test.xlsx\")\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "test_ds = test_ds.remove_columns([c for c in test_ds.column_names if c.startswith(\"__\")])\n",
    "\n",
    "print(\"Held-out test set size:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fadef3f-9b73-49d9-9da0-ed1c55fc7364",
   "metadata": {},
   "source": [
    "## Fine-Tuning DistilBERT on Our Toy Sentiment Data\n",
    "\n",
    "This cell puts everything together—from loading our small Excel-based datasets all the way through to training, saving, and evaluating a DistilBERT sentiment classifier. Below is a step-by-step explanation of what happens and the key parameters used.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Load the Data  \n",
    "- **`training.xlsx`** (40 examples, 20 positive / 20 negative) is read into `train_df`.  \n",
    "- **`test.xlsx`** (200 examples, 100 positive / 100 negative) is read into `test_df`.  \n",
    "\n",
    "### 2) Wrap in Hugging Face Datasets  \n",
    "- We convert `train_df` and `test_df` into `Dataset` objects so we can apply Hugging Face utilities like `train_test_split`, `.map()`, and `.set_format()`.\n",
    "\n",
    "### 3) Split for Training/Validation  \n",
    "- **Train/validation split**: We use an 80/20 split on the 40-example set, yielding **32 training** and **8 validation** examples.  \n",
    "- A fixed random seed (`seed=42`) ensures that this split is reproducible.\n",
    "\n",
    "### 4) Tokenisation  \n",
    "- We load the **DistilBERT tokenizer** (`distilbert-base-uncased`), which uses WordPiece tokenisation with a 30 522-token vocabulary.  \n",
    "- Sentences are padded or truncated to **64 tokens** to keep each batch small and fast on CPU.  \n",
    "\n",
    "### 5) Formatting for PyTorch  \n",
    "- The original column `label` is renamed to `labels` (what the Trainer expects).  \n",
    "- We call `.set_format(type=\"torch\", columns=[…])` so that each batch yields PyTorch tensors for `input_ids`, `attention_mask`, and `labels`.\n",
    "\n",
    "### 6) Model, Metric and Data Collator  \n",
    "- **Model**: We load **DistilBertForSequenceClassification** (a 6-layer, 768-hidden-size, 12-head Transformer with ∼66 million parameters) pre-trained on general English.  \n",
    "- **Classification head (MLP)**:  \n",
    "  - **pre_classifier**: a single hidden layer of size 768, followed by GELU activation and dropout (0.1).  \n",
    "  - **classifier**: the output layer mapping those 768 activations down to 2 logits (positive vs. negative).  \n",
    "  - **Thought for a couple of seconds**  \n",
    "    Not quite two hidden layers of 768 each—instead there is **one** hidden layer (768 units) plus the final output layer (2 units).  \n",
    "- **Metric**: We use the `evaluate` library’s **accuracy** for both validation and test.  \n",
    "- **Data collator**: `DataCollatorWithPadding` batches and pads examples on the fly.\n",
    "\n",
    "### 7) Training Arguments  \n",
    "| Argument                         | Value             | Purpose                                   |\n",
    "|----------------------------------|-------------------|-------------------------------------------|\n",
    "| `output_dir=\"distilbert_finetuned\"` | —               | Directory to save checkpoints & final model |\n",
    "| `num_train_epochs=5`             | 5 epochs          | Number of full passes over the 32 examples|\n",
    "| `per_device_train_batch_size=4`  | 4 examples/batch  | Small batch size to fit CPU memory        |\n",
    "| `learning_rate=2e-5`             | 2 × 10⁻⁵          | Standard fine-tuning learning rate        |\n",
    "| `logging_steps=10`               | every 10 steps    | Log training loss/metrics every 10 steps  |\n",
    "| `save_steps=10`                  | every 10 steps    | Save checkpoint frequently for safety     |\n",
    "| `no_cuda=True`                   | force CPU         | Ensure no GPU is used                     |\n",
    "\n",
    "### 8) Trainer Setup  \n",
    "- We instantiate the Hugging Face **`Trainer`** with all components:  \n",
    "  - **`model`**: our DistilBERT classifier  \n",
    "  - **`args`**: the training arguments above  \n",
    "  - **`train_dataset`**, **`eval_dataset`**: our 32/8 split  \n",
    "  - **`tokenizer`** & **`data_collator`**: for preparing batches  \n",
    "  - **`compute_metrics`**: to report accuracy on validation each epoch  \n",
    "\n",
    "### 9) Training  \n",
    "- `trainer.train()` runs 5 epochs over 32 examples each, validating on the 8-example set after every epoch.\n",
    "\n",
    "### 10) Saving the Model  \n",
    "- After training, the fine-tuned model and tokenizer are both saved into the directory **`distilbert_finetuned/`**:  \n",
    "\n",
    "distilbert_finetuned/\n",
    "├── config.json\n",
    "├── pytorch_model.bin ← fine-tuned weights\n",
    "├── tokenizer_config.json\n",
    "├── vocab.txt ← tokenizer vocabulary\n",
    "└── … ← other tokenizer files\n",
    "\n",
    "\n",
    "### 11) Final Evaluation  \n",
    "- We print **validation** accuracy (on the 8 examples seen only between epochs) and **held-out test** accuracy (200 examples never used during training nor validation).\n",
    "\n",
    "---\n",
    "\n",
    "**Model Specification Recap**  \n",
    "- **Architecture**:  \n",
    "- DistilBERT Base (6 Transformer layers, hidden size 768, 12 self-attention heads per layer)  \n",
    "- Classification head: one 768-unit hidden layer (GELU + dropout 0.1) → 2-unit output  \n",
    "- **Pre-training**: Masked LM + distillation from BERT-Base on Wikipedia + BookCorpus  \n",
    "- **Total parameters**: ∼66 million (including MLP head)  \n",
    "\n",
    "This cell encapsulates the full fine-tuning pipeline—from raw Excel data to a saved, ready-to-deploy sentiment classifier (`distilbert_finetuned`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4125cd3d-8286-47a6-9192-3704c7d181fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575bbd349a6e4260834cbae8781807b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9531e7dc61a4bb5a8beb40cda1e1a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8d3b697c3841e6bda3c30b45c7681c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\IAGhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1577: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\IAGhe\\AppData\\Local\\Temp\\ipykernel_16152\\2484884860.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.683200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.572300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.552500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7560411691665649, 'eval_accuracy': 0.375, 'eval_runtime': 0.2213, 'eval_samples_per_second': 36.143, 'eval_steps_per_second': 4.518, 'epoch': 5.0}\n",
      "\n",
      "Held-out test set results:\n",
      "{'eval_loss': 0.6634376645088196, 'eval_accuracy': 0.58, 'eval_runtime': 4.7849, 'eval_samples_per_second': 41.798, 'eval_steps_per_second': 5.225, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from evaluate import load as load_metric\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# ---- 1) Load Excel files into DataFrames ----\n",
    "train_df = pd.read_excel(\"training.xlsx\")  # 40 examples\n",
    "test_df  = pd.read_excel(\"test.xlsx\")      # 200 examples\n",
    "\n",
    "# ---- 2) Create HF Datasets ----\n",
    "full_train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds       = Dataset.from_pandas(test_df)\n",
    "\n",
    "# ---- 3) Split full_train_ds into train/val ----\n",
    "split    = full_train_ds.train_test_split(test_size=0.2, seed=42)\n",
    "train_ds = split[\"train\"]  # 32 examples\n",
    "val_ds   = split[\"test\"]   #  8 examples\n",
    "\n",
    "# ---- 4) Tokeniser & tokenisation function ----\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "val_ds   = val_ds.map(tokenize_fn, batched=True)\n",
    "test_ds  = test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "# ---- 5) Rename label → labels & set PyTorch format ----\n",
    "train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "val_ds   = val_ds.rename_column(\"label\", \"labels\")\n",
    "test_ds  = test_ds.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_ds.set_format(  type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_ds.set_format( type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# ---- 6) Prepare model, metric, data collator ----\n",
    "model  = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# ---- 7) TrainingArguments ----\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert_finetuned\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "# ---- 8) Trainer setup ----\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ---- 9) Train ----\n",
    "trainer.train()\n",
    "\n",
    "# ---- 10) Save fine-tuned model & tokenizer ----\n",
    "# This writes model weights, config, and tokenizer files into `distilbert_finetuned/`\n",
    "trainer.save_model(training_args.output_dir)\n",
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# ---- 11) Evaluate ----\n",
    "print(\"\\nValidation set results:\")\n",
    "print(trainer.evaluate(eval_dataset=val_ds))\n",
    "\n",
    "print(\"\\nHeld-out test set results:\")\n",
    "print(trainer.evaluate(eval_dataset=test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0965ea0d-429b-4070-b504-9f7f3ef16ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
